{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf5987d6",
   "metadata": {},
   "source": [
    "# <center> Meters Reader (using YOLO and OpenCV) </center>  \n",
    "\n",
    "## Table of Contents <a name = 'content'></a>  \n",
    "\n",
    "1. [Problem statement](#problem)   \n",
    "2. [Project navigation](#navigation)   \n",
    "3. [Description of solution logic](#logic)    \n",
    "4. [Example of a meter read](#performance_example)     \n",
    "5. [How to run code on your device](#run_code)    \n",
    "6. [Usage](#usage)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7932a2b5",
   "metadata": {},
   "source": [
    "## Problem statement <a name = 'problem'></a>\n",
    "\n",
    "[Table of Contents](#content)\n",
    "\n",
    "\n",
    "The goal of this project is to read meter readings from a photo using **YOLOv8** and **OpenCV** library. To solve this task, I used a [dataset from Kaggle](https://www.kaggle.com/datasets/tapakah68/yandextoloka-water-meters-dataset) that contains 1244 meter images and masks. The sample data from this set looks like this:   \n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "\n",
    "  <div style=\"text-align: center; margin-right: 20px; margin-left: 150px;\">\n",
    "    <img src=\"pics/img_exml.jpg\" alt=\"Image 1\" style=\"width: 300px;\"/>\n",
    "    <p style=\"margin-left: 100px;\">Image example</p>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"text-align: center; margin-left: 20px;\">\n",
    "    <img src=\"pics/mask_exml.jpg\" alt=\"Image 2\" style=\"width: 300px;\"/>\n",
    "    <p style=\"margin-left: 100px;\">Mask example</p>\n",
    "  </div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d7533",
   "metadata": {},
   "source": [
    "## Project navigation <a name = 'navigation'></a>  \n",
    "\n",
    "[Table of Contents](#content)   \n",
    "\n",
    "* `data` folder contains original dataset from Kaggle, data prepared for training YOLO segmentation and detection models, data of each preparation step, annotation labels for detection model training and my meter photos that I used to test program;\n",
    "* `models` folder contains trained segmentation and detection models, that are neccessary for making predictions;\n",
    "* `pics` folder contains the supporting images for this notebook;\n",
    "* `predictions` folder contains couple of examples of how program reads meter readings;    \n",
    "* `utils.py` file contains all the functions that I use for data preparation, models training and making prediction;  \n",
    "* `main.py` file perform data preparations, models training and making prediction using functions from the `utils.py`;\n",
    "* `yolo_segment.yaml` is the file that is needed for the YOLO segmentation model to understand where to look for training data. It contains directory paths and classes;\n",
    "* `yolo_detect.yaml` is the file that is needed for the YOLO detection model to understand where to look for training data. It contains directory paths and classes;\n",
    "* `yolov8n-seg.pt` is a YOLO segmentation model with trained weights that I use as the backbone;\n",
    "* `yolov8s.pt` is a YOLO detection model with trained weights that I use as the backbone;\n",
    "* `requirements.txt` is a list of required libraries and their versions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9a6da5",
   "metadata": {},
   "source": [
    "## Description of solution logic <a name = 'logic'></a>  \n",
    "\n",
    "[Table of Contents](#content)\n",
    "\n",
    "There are 3 main steps in the solution:\n",
    "1) [Training the segmentation model to identify the area with numbers in the meter image](#segment);\n",
    "2) [Training an object detection model to classify each number in that zone](#detect);    \n",
    "3) [Converting the prediction of the object detection model to the meter reading and saving the results](#pred2num)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2411d733",
   "metadata": {},
   "source": [
    "### 1. Training the segmentation model <a name = 'segment'></a>        \n",
    "\n",
    "[Back to description plan](#logic)\n",
    "\n",
    "As mentioned, I used the YOLOv8 models by ultralytics. In order to achieve faster training and higher model accuracy, I used transfer learning approach.   \n",
    "\n",
    "Before training the YOLO segmentation model, there are a few preparatory steps to follow.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f67db",
   "metadata": {},
   "source": [
    "First I converted the data structure to the form that YOLO requires. You can see the schema below:   \n",
    "\n",
    "<div style=\"text-align: center; margin-right: 20px; margin-left: 70px;\">\n",
    "    <img src=\"pics/dir_struc.jpg\" alt=\"Image 1\" style=\"width: 200px;\"/>\n",
    "    <p style=\"margin-left: 400px;\"></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934f234c",
   "metadata": {},
   "source": [
    "Next, I converted the masks to label format. This is a format that is also required for YOLO to work correctly.    \n",
    "\n",
    "The last step in the preparation is to create a `yolo_segment.yaml` file. This contains path information for training, validation, and test data that helps YOLO find the data in the file system. The `.yaml` file looks like this:    \n",
    "\n",
    "<div style=\"text-align: center; margin-right: 20px; margin-left: 70px;\">\n",
    "    <img src=\"pics/segment_yaml.jpg\" alt=\"Image 1\" style=\"width: 700px;\"/>\n",
    "    <p style=\"margin-left: 400px;\"></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b00f82",
   "metadata": {},
   "source": [
    "After all the preparations are done, I loaded the model `yolov8n-seg.pt` with trained weights. This is my backbone. Next, I trained the model on meter images for 10 epochs, and took the `best.pt` model to use.   \n",
    "\n",
    "Resulting segmentation model gives such results: \n",
    "\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "\n",
    "  <div style=\"text-align: center; margin-right: 20px; margin-left: 150px;\">\n",
    "    <img src=\"pics/img_exml.jpg\" alt=\"Image 1\" style=\"width: 300px;\"/>\n",
    "    <p style=\"margin-left: 50px;\">Image before segmentation</p>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"text-align: center; margin-left: 20px;\">\n",
    "    <img src=\"pics/segmented.jpg\" alt=\"Image 2\" style=\"width: 300px;\"/>\n",
    "    <p style=\"margin-left: 100px;\">Segmented image</p>\n",
    "  </div>\n",
    "\n",
    "</div>  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f01b920",
   "metadata": {},
   "source": [
    "### 2. Training the object detection model <a name = 'detect'></a>        \n",
    "\n",
    "[Back to description plan](#logic)\n",
    "\n",
    "The preparations for training the object detection model are more complicated. I need to get same numbers format on each meter in order to train a versatile model.    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344ff18e",
   "metadata": {},
   "source": [
    "First, I apply segmentation masks to the meter images and save the results to the `data/masked_by_YOLO` folder.      \n",
    "\n",
    "Example of masked image:   \n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "\n",
    "  <div style=\"text-align: center; margin-right: 20px; margin-left: 150px;\">\n",
    "    <img src=\"pics/img_exml.jpg\" alt=\"Image 1\" style=\"width: 300px;\"/>\n",
    "    <p style=\"margin-left: 90px;\">Original image</p>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"text-align: center; margin-left: 20px;\">\n",
    "    <img src=\"pics/masked.jpg\" alt=\"Image 2\" style=\"width: 300px;\"/>\n",
    "    <p style=\"margin-left: 80px;\">Image masked by YOLO</p>\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbbcd1c",
   "metadata": {},
   "source": [
    "Now my task is to carefully cut out the area with numbers.   \n",
    "To do this, I first find the contour of the area with the numbers: \n",
    "\n",
    "<div style=\"text-align: center; margin-right: 20px; margin-left: 70px;\">\n",
    "    <img src=\"pics/numbers_contour.jpg\" alt=\"Image 1\" style=\"width: 400px;\"/>\n",
    "    <p style=\"margin-left: 375px;\">Found numbers contour</p>\n",
    "</div>   \n",
    "\n",
    "I then find the minimum rectangle that bounds this contour:    \n",
    "<div style=\"text-align: center; margin-right: 20px; margin-left: 70px;\">\n",
    "    <img src=\"pics/rectangle.jpg\" alt=\"Image 1\" style=\"width: 400px;\"/>\n",
    "    <p style=\"margin-left: 360px;\">Found bounding rectangle</p>\n",
    "</div>  \n",
    "\n",
    "\n",
    "Now I find the vertices of this rectangle in order *top left -> top right -> bottom left -> bottom right*:       \n",
    "\n",
    "<div style=\"text-align: center; margin-right: 20px; margin-left: 70px;\">\n",
    "    <img src=\"pics/verticles.jpg\" alt=\"Image 1\" style=\"width: 400px;\"/>\n",
    "    <p style=\"margin-left: 400px;\">Found verticles</p>\n",
    "</div> \n",
    "\n",
    "\n",
    "Now I check what is bigger: the distance between vertices 0 and 1 or between vertices 0 and 2 (is numbers zone rotated horizontally or vertically). Based on the results of the check, I rotate the image by stretching the corners of the rectangle to the corners of the image and resize it.   \n",
    "\n",
    "The logic of the operation looks like this:   \n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "\n",
    "  <div style=\"text-align: center; margin-right: 20px; margin-left: 150px;\">\n",
    "    <img src=\"pics/hor_marks.jpg\" alt=\"Image 1\" style=\"width: 300px;\"/>\n",
    "    <p style=\"margin-left: 60px;\">Horizontally rotated numbers</p>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"text-align: center; margin-left: 20px;\">\n",
    "    <img src=\"pics/ver_marks.jpg\" alt=\"Image 2\" style=\"width: 225px;\"/>\n",
    "    <p style=\"margin-left: 40px;\">Vertically rotated numbers</p>\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "**Pay attention to where the corners of the number area \"go\"!**   \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5389ebf7",
   "metadata": {},
   "source": [
    "After rotating with this strategy and resizing, I got cut out areas with numbers in the same format:  \n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "\n",
    "  <div style=\"text-align: center; margin-right: 20px; margin-left: 150px;\">\n",
    "    <img src=\"pics/cropped_hor.jpg\" alt=\"Image 1\" style=\"width: 300px;\"/>\n",
    "    <p style=\"margin-left: 25px;\">Cropped numbers from left image above</p>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"text-align: center; margin-left: 20px;\">\n",
    "    <img src=\"pics/cropped_ver.jpg\" alt=\"Image 2\" style=\"width: 300px;\"/>\n",
    "    <p style=\"margin-left: 25px;\">Cropped numbers from right image above</p>\n",
    "  </div>\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "Cropped images are saved in `data/cropped_by_YOLO` folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2b582d",
   "metadata": {},
   "source": [
    "Unfortunately, the original dataset don't have the annotated data needed to train the number classifier, so I had to do everything manually. I used the free service [CVAT.ai](https://www.cvat.ai/) to annotate the data. On each image, I marked out the area where the digit is located and assigned each digit a class from 0 to 9. In total, just over 9,000 figures were noted.   \n",
    "\n",
    "An example of the image annotation looks like this. Here, each color corresponds to a specific class:   \n",
    "\n",
    "<div style=\"text-align: center; margin-right: 20px; margin-left: 70px;\">\n",
    "    <img src=\"pics/annot_exml.jpg\" alt=\"Image 1\" style=\"width: 400px;\"/>\n",
    "    <p style=\"margin-left: 370px;\">Example of annotation</p>\n",
    "</div> \n",
    "\n",
    "*I chose a criterion for data annotation - if I can definitely recognize a digit, I annotate it, even if only part of the digit is visible.*\n",
    "\n",
    "Some images turned out upside down. This was due to the unusual angle of the meter photo. Here is an example of such an image:   \n",
    "\n",
    "<div style=\"text-align: center; margin-right: 20px; margin-left: 70px;\">\n",
    "    <img src=\"pics/upside-down.jpg\" alt=\"Image 1\" style=\"width: 400px;\"/>\n",
    "    <p style=\"margin-left: 340px;\">Example of upside-down image</p>\n",
    "</div> \n",
    "\n",
    "I ignored the upside down images, as they are an absolute minority.\n",
    "\n",
    "I uploaded the annotated data in the format required for YOLO to work. It is important to remember that annotated data saved in `data/annotated_numbers_for_YOLO_detection/labels` is only suitable for data segmented with `yolov8n-seg.pt` for 10 epochs (`best.pt`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b541a3c7",
   "metadata": {},
   "source": [
    "After that, I put the classifier training data into the same structure as for the segmentation model:  \n",
    "\n",
    "\n",
    "<div style=\"text-align: center; margin-right: 20px; margin-left: 70px;\">\n",
    "    <img src=\"pics/dir_struc.jpg\" alt=\"Image 1\" style=\"width: 200px;\"/>\n",
    "    <p style=\"margin-left: 400px;\"></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9f8b74",
   "metadata": {},
   "source": [
    "The last step in the preparation is to create a `yolo_detect.yaml` file. This contains path information for training, validation, and test data that helps YOLO find the data in the file system. The `.yaml` file looks like this:    \n",
    "\n",
    "<div style=\"text-align: center; margin-right: 20px; margin-left: 70px;\">\n",
    "    <img src=\"pics/detect_yaml.jpg\" alt=\"Image 1\" style=\"width: 550px;\"/>\n",
    "    <p style=\"margin-left: 400px;\"></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3d18c6",
   "metadata": {},
   "source": [
    "All preparations are made. Now I train `yolov8s.pt` for 20 epochs, and take the `best.pt` model for predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56dad02",
   "metadata": {},
   "source": [
    "### Object detection model prediction to meter readings <a name = 'pred2num'></a>  \n",
    "\n",
    "[Back to description plan](#logic)\n",
    "\n",
    "The last thing left to do is to convert the model prediction into a number (meter readings).  \n",
    "\n",
    "To do this, I sort the detected digits in order from left to right and remove all zeros from the beginning. \n",
    "\n",
    "It happens that the model recognizes two digits on the same vertical level. This happens most often on the last digit - where two digits can be clearly visible, as in the example below:    \n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "\n",
    "  <div style=\"text-align: center; margin-right: 20px; margin-left: 150px;\">\n",
    "    <img src=\"pics/duplicate_num.jpg\" alt=\"Image 1\" style=\"width: 300px;\"/>\n",
    "    <p style=\"margin-left: 100px;\">Cropped image</p>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"text-align: center; margin-left: 20px;\">\n",
    "    <img src=\"pics/duplicate_det.jpg\" alt=\"Image 2\" style=\"width: 300px;\"/>\n",
    "    <p style=\"margin-left: 75px;\">Detection model prediction</p>\n",
    "  </div>\n",
    "\n",
    "</div>   \n",
    "\n",
    "I leave the digit that has the larger height of the bounding box. Since the height of the digit is greater, it is more visible and it is a more honest meter reading.     \n",
    "In this particular example, the height of the bounding box of the 2 is greater than the height of the bounding box of the 1, so we leave the 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db18c9f",
   "metadata": {},
   "source": [
    "The last issue to solve is the floating comma. Meter readings have integer and fractional parts, but there is no single variant of how they are visually separated. They can be separated by a comma, by the color of the digits, by the color of the background, by different fonts, so it's not clear how to create a universal recognition algorithm.   \n",
    "\n",
    "\n",
    "However, I noticed that meters fall into two types: either they have only 5 digits and no fractional part, or they have more than 6 digits and the fractional part takes 3 digits.  \n",
    "\n",
    "Two types of meters:\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "\n",
    "  <div style=\"text-align: center; margin-right: 20px; margin-left: 150px;\">\n",
    "    <img src=\"pics/3_float.jpg\" alt=\"Image 1\" style=\"width: 300px;\"/>\n",
    "    <p style=\"margin-left: 40px;\">More than 6 digits, 3 digits fract. part</p>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"text-align: center; margin-left: 20px;\">\n",
    "    <img src=\"pics/5_int.jpg\" alt=\"Image 2\" style=\"width: 300px;\"/>\n",
    "    <p style=\"margin-left: 85px;\">5 digits, no fract. part</p>\n",
    "  </div>\n",
    "\n",
    "</div>   \n",
    "\n",
    "I don't know if this rule works on all the meters in the world, but in my dataset of 1244 images, there is not a single violation of this rule. Moreover, the meters in my house also follow it. You can check the meters in your home)   \n",
    "\n",
    "So I just check how many digits the program detected. If it is less than 6, I take the reading as a integer. If there are more than 6 digits, I divide the integer by 1000, shifting the comma by 3 digits.   \n",
    "\n",
    "And that's it. The project is done!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30aa202",
   "metadata": {},
   "source": [
    "## Example of a meter read <a name = 'performance_example'></a>  \n",
    "\n",
    "[Table of Contents](#content) \n",
    "\n",
    "Below you can see examples of how the program works on the photos of the meters from my apartment:   \n",
    "\n",
    "<div style=\"display: flex; justify-content: space-around;\">\n",
    "\n",
    "  <div style=\"text-align: center; margin-right: 20px; margin-left: 150px;\">\n",
    "    <img src=\"pics/pred1.jpg\" alt=\"Image 1\" style=\"width: 300px;\"/>\n",
    "    <p style=\"margin-left: 40px;\"></p>\n",
    "  </div>\n",
    "  \n",
    "  <div style=\"text-align: center; margin-left: 20px;\">\n",
    "    <img src=\"pics/pred2.jpg\" alt=\"Image 2\" style=\"width: 300px;\"/>\n",
    "    <p style=\"margin-left: 85px;\"></p>\n",
    "  </div>\n",
    "\n",
    "</div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b8a071",
   "metadata": {},
   "source": [
    "## How to run code on your device <a name = 'run_code'></a>  \n",
    "\n",
    "[Table of Contents](#content) \n",
    "\n",
    "To install all required dependencies, run the `requirements.txt` file from the root directory of the project:    \n",
    "> pip install -r requirements.txt   \n",
    "\n",
    "The `main.py` file contains variables that you can set to `True` to reproduce by yourself *data preparation for segmentation model training, segmentation model training, data preparation for object detection model training, object detection model training and prediction on the meter image.*     \n",
    "\n",
    "<div style=\"text-align: center; margin-right: 20px; margin-left: 70px;\">\n",
    "    <img src=\"pics/hyper_vars.jpg\" alt=\"Image 1\" style=\"width: 550px;\"/>\n",
    "    <p style=\"margin-left: 400px;\"></p>\n",
    "</div>\n",
    "\n",
    "If you want to train models on your own, remember:   \n",
    "* Data for training segmentation model is stored in `data/YOLO_segmentation_data`;\n",
    "* Data for training object detection model is stored in `data/YOLO_detection_data`;\n",
    "* Replace the `path` variable in `.yaml` files with the **absolute** path to `YOLO_segmentation_data` and `YOLO_detection_data` in your file system;\n",
    "* If you don't have GPU on your computer and you don't want training to last forever, use GoogleColab or something similar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b82fdd2",
   "metadata": {},
   "source": [
    "## Usage <a name = 'usage'></a>  \n",
    "\n",
    "[Table of Contents](#content)  \n",
    "\n",
    "If you want to read the meter readings from your image, here's what you need to do:  \n",
    "1) Set `MAKE_PREDICTION_ON_IMAGE` in `main.py` to `True`;\n",
    "2) Replace the values of the `image_path` and `path_to_save_predictions` variables (lines 88 and 91 in `main.py`) with the path to your image and the path where you want to save the prediction, respectively;\n",
    "3) Execute the `main.py` file:   \n",
    "> python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f51585",
   "metadata": {},
   "source": [
    "I hope you like the solution. Enjoy coding and have a good day!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
